{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp ms_data_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base MS Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from alphabase.io.hdf import HDF_File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class MSData_Base:\n",
    "    \"\"\"\n",
    "    The base data structure for MS Data, other MSData loader inherit\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    centroided : bool, optional\n",
    "        if peaks will be centroided after loading, \n",
    "        by default True\n",
    "    \"\"\"\n",
    "    def __init__(self, centroided:bool=True):\n",
    "        # A spectrum contains peaks\n",
    "        self.spectrum_df:pd.DataFrame = pd.DataFrame()\n",
    "        # A peak contains mz, intensity, and ...\n",
    "        self.peak_df:pd.DataFrame = pd.DataFrame()\n",
    "        self._raw_file_path = ''\n",
    "        self.centroided = centroided\n",
    "        self.creation_time = ''\n",
    "        self.file_type = ''\n",
    "\n",
    "    @property\n",
    "    def raw_file_path(self)->str:\n",
    "        return self._raw_file_path\n",
    "\n",
    "    @raw_file_path.setter\n",
    "    def raw_file_path(self, _path:str):\n",
    "        self._raw_file_path = _path\n",
    "\n",
    "    def import_raw(self, _path:str):\n",
    "        self.raw_file_path = _path\n",
    "        raw_data = self._import(_path)\n",
    "        self._set_dataframes(raw_data)\n",
    "        self._check_df()\n",
    "\n",
    "    def load_raw(self, _path:str):\n",
    "        self.import_raw(_path)\n",
    "\n",
    "    def _save_meta_to_hdf(self, hdf:HDF_File):\n",
    "        hdf.ms_data.creation_time = self.creation_time\n",
    "        hdf.ms_data.raw_file_path = self.raw_file_path\n",
    "        hdf.ms_data.file_type = self.file_type\n",
    "        hdf.ms_data.centroided = self.centroided\n",
    "\n",
    "    def _load_meta_from_hdf(self, hdf:HDF_File):\n",
    "        self.creation_time = hdf.ms_data.creation_time\n",
    "        self.raw_file_path = hdf.ms_data.raw_file_path\n",
    "        self.file_type = hdf.ms_data.file_type\n",
    "        self.centroided = hdf.ms_data.centroided\n",
    "\n",
    "    def save_hdf(self, _path:str):\n",
    "        hdf = HDF_File(\n",
    "            _path, read_only=False,\n",
    "            truncate=True, delete_existing=True\n",
    "        )\n",
    "\n",
    "        hdf.ms_data = {\n",
    "            'spectrum_df': self.spectrum_df,\n",
    "            'peak_df': self.peak_df\n",
    "        }\n",
    "\n",
    "        self._save_meta_to_hdf(hdf)\n",
    "        \n",
    "\n",
    "    def load_hdf(self, _path:str):\n",
    "        hdf = HDF_File(\n",
    "            _path, read_only=True,\n",
    "            truncate=False, delete_existing=False\n",
    "        )\n",
    "\n",
    "        self.spectrum_df = hdf.ms_data.spectrum_df.values\n",
    "        self.peak_df = hdf.ms_data.peak_df.values\n",
    "\n",
    "        self._load_meta_from_hdf(hdf)\n",
    "\n",
    "    def reset_spec_idxes(self):\n",
    "        self.spectrum_df.reset_index(drop=True, inplace=True)\n",
    "        self.spectrum_df['spec_idx'] = self.spectrum_df.index.values\n",
    "\n",
    "    def _import(self, _path):\n",
    "        raise NotImplementedError(\n",
    "            f\"{self.__class__} must implement `_import()`\"\n",
    "        )\n",
    "    \n",
    "    def _set_dataframes(self, raw_data):\n",
    "        raise NotImplementedError(\n",
    "            f\"{self.__class__} must implement `_set_dataframes()`\"\n",
    "        )\n",
    "\n",
    "    def _read_creation_time(self, raw_data):\n",
    "        pass\n",
    "\n",
    "    def _check_df(self):\n",
    "        self._check_rt()\n",
    "        # self._check_mobility()\n",
    "        self._check_precursor_mz()\n",
    "    \n",
    "    def _check_rt(self):\n",
    "        assert 'rt' in self.spectrum_df.columns\n",
    "        self.spectrum_df['rt_sec'] = self.spectrum_df.rt*60\n",
    "\n",
    "    def _check_mobility(self):\n",
    "        if 'mobility' not in self.spectrum_df.columns:\n",
    "            self.spectrum_df['mobility'] = 0.0\n",
    "\n",
    "    def _check_precursor_mz(self):\n",
    "        if 'isolation_lower_mz' not in self.spectrum_df.columns:\n",
    "            self.spectrum_df['isolation_lower_mz'] = -1.0\n",
    "            self.spectrum_df['isolation_upper_mz'] = -1.0\n",
    "        if 'precursor_mz' not in self.spectrum_df.columns:\n",
    "            self.spectrum_df['precursor_mz'] = -1.0\n",
    "\n",
    "    def create_spectrum_df(self,\n",
    "        spectrum_num:int,\n",
    "    ):\n",
    "        self.spectrum_df = pd.DataFrame(\n",
    "            index=np.arange(spectrum_num, dtype=np.int64)\n",
    "        )\n",
    "        self.spectrum_df['spec_idx'] = self.spectrum_df.index.values\n",
    "\n",
    "    def set_peaks_by_cat_array(self, \n",
    "        mz_array:np.ndarray, \n",
    "        intensity_array:np.ndarray,\n",
    "        peak_start_indices:np.ndarray,\n",
    "        peak_end_indices:np.ndarray,\n",
    "    ):\n",
    "        self.peak_df = pd.DataFrame({\n",
    "            'mz': mz_array,\n",
    "            'intensity': intensity_array\n",
    "        })\n",
    "        self.spectrum_df['peak_start_idx'] = peak_start_indices\n",
    "        self.spectrum_df['peak_end_idx'] = peak_end_indices\n",
    "\n",
    "    def set_peaks_by_array_list(self,\n",
    "        mz_array_list:list,\n",
    "        intensity_array_list:list,\n",
    "    ):\n",
    "        indices = index_ragged_list(mz_array_list)\n",
    "        self.set_peaks_by_cat_array(\n",
    "            np.concatenate(mz_array_list),\n",
    "            np.concatenate(intensity_array_list),\n",
    "            indices[:-1],\n",
    "            indices[1:]\n",
    "        )\n",
    "\n",
    "    def add_column_in_spec_df(self, \n",
    "        column_name:str, \n",
    "        values:np.ndarray, \n",
    "        spec_idxes:np.ndarray = None,\n",
    "        dtype:np.dtype=np.float64, na_value=np.nan,\n",
    "    ):\n",
    "        if spec_idxes is None:\n",
    "            self.spectrum_df[\n",
    "                column_name\n",
    "            ] = np.array(values, dtype=dtype)\n",
    "        else:\n",
    "            self.spectrum_df.loc[\n",
    "                spec_idxes, column_name\n",
    "            ] = values\n",
    "            self.spectrum_df[column_name].fillna(\n",
    "                na_value, inplace=True\n",
    "            )\n",
    "            self.spectrum_df[\n",
    "                column_name\n",
    "            ] = self.spectrum_df[column_name].astype(dtype)\n",
    "\n",
    "    def add_column_in_df_by_thermo_scan_num(self, \n",
    "        column_name:str, \n",
    "        values:np.ndarray, \n",
    "        scan_nums:np.ndarray,\n",
    "        dtype:np.dtype=np.float64, na_value=np.nan,\n",
    "    ):\n",
    "        self.add_column_in_spec_df(\n",
    "            column_name, values,\n",
    "            scan_nums-1, \n",
    "            dtype, na_value\n",
    "        )\n",
    "\n",
    "    def get_peaks(self, spec_idx):\n",
    "        start, end = self.spectrum_df[[\n",
    "            'peak_start_idx','peak_end_idx'\n",
    "        ]].values[spec_idx,:]\n",
    "        return (\n",
    "            self.peak_df.mz.values[start:end],\n",
    "            self.peak_df.intensity.values[start:end],\n",
    "        )\n",
    "\n",
    "    def set_precursor_mz(self, \n",
    "        precursor_mz_values:np.ndarray,\n",
    "        spec_idxes:np.ndarray=None, \n",
    "    ):\n",
    "        self.add_column_in_spec_df(\n",
    "            'precursor_mz', \n",
    "            precursor_mz_values, \n",
    "            spec_idxes, np.float64, -1.0\n",
    "        )\n",
    "    \n",
    "    def set_precursor_mz_windows(self,\n",
    "        precursor_lower_mz_values:np.ndarray,\n",
    "        precursor_upper_mz_values:np.ndarray,\n",
    "        spec_idxes:np.ndarray=None, \n",
    "    ):\n",
    "        self.add_column_in_spec_df(\n",
    "            'isolation_lower_mz',\n",
    "            precursor_lower_mz_values, \n",
    "            spec_idxes, np.float64, -1.0\n",
    "        )\n",
    "        self.add_column_in_spec_df(\n",
    "            'isolation_upper_mz',\n",
    "            precursor_upper_mz_values, \n",
    "            spec_idxes, np.float64, -1.0\n",
    "        )\n",
    "\n",
    "def index_ragged_list(ragged_list: list)  -> np.ndarray:\n",
    "    \"\"\"Create lookup indices for a list of arrays for concatenation.\n",
    "\n",
    "    Args:\n",
    "        value (list): Input list of arrays.\n",
    "\n",
    "    Returns:\n",
    "        indices: A numpy array with indices.\n",
    "    \"\"\"\n",
    "    indices = np.zeros(len(ragged_list) + 1, np.int64)\n",
    "    indices[1:] = [len(i) for i in ragged_list]\n",
    "    indices = np.cumsum(indices, dtype=np.int64)\n",
    "\n",
    "    return indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class MSData_HDF(MSData_Base):\n",
    "    def import_raw(self, _path:str):\n",
    "        self.raw_file_path = _path\n",
    "        self.load_hdf(_path)\n",
    "\n",
    "class MSReaderProvider:\n",
    "    \"\"\"Factory class to register and get MS Readers\"\"\"\n",
    "    def __init__(self):\n",
    "        self.ms_reader_dict = {}\n",
    "\n",
    "    def register_reader(self, ms2_type:str, reader_class):\n",
    "        self.ms_reader_dict[ms2_type.lower()] = reader_class\n",
    "\n",
    "    def get_reader(\n",
    "        self, file_type:str, \n",
    "        *, \n",
    "        centroided:bool=True,\n",
    "        **kwargs\n",
    "    )->MSData_Base:\n",
    "        file_type = file_type.lower()\n",
    "        if file_type not in self.ms_reader_dict: return None\n",
    "        else: return self.ms_reader_dict[file_type](\n",
    "            centroided=centroided, **kwargs\n",
    "        )\n",
    "\n",
    "ms_reader_provider = MSReaderProvider()\n",
    "ms_reader_provider.register_reader('alpharaw', MSData_HDF)\n",
    "ms_reader_provider.register_reader('alpharaw_hdf', MSData_HDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MSData_HDF>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "ms_reader_provider.get_reader('alpharaw')\n",
    "ms_reader_provider.get_reader('alpharaw_hdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
